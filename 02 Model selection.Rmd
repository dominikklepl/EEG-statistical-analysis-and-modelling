---
title: "Modelling and selection"
author: "Dominik Klepl"
date: "12/4/2019"
output: word_document
---

```{r setup, include=FALSE}
pacman::p_load(ggplot2, ggthemes, tidyr, patchwork)
```


Now that we kinda understand our data we can start modelling. We'll use two approaches to identidy the best model structure.

1. Forward subset selection to identify the best model by minimizing MSE. We'll split the data into training and testing set using 80:20 ratio.
2. Use AIC OR BIC to identify the best model.

Hopefully both methods will converge on the same model structure.

Of course, we need to load the data first.
```{r}
data = read.csv("data/x_y.csv", header = F)
colnames(data) = c("x", "y")
```


# Forward subset selection

First, create matrix X with all possible predictors, i.e. intercept and x^1 to x^5
```{r echo=F}
X = cbind(rep(1, nrow(data)), #intercept
          data$x,
          data$x^2,
          data$x^3,
          data$x^4,
          data$x^5
          ) 

colnames(X) = c("intercept", "x1", "x2", "x3", "x4", "x5")

#sanity check
cat("Is X matrix? ")
class(X) == "matrix"

cat("Show first 5 rows of X\n")
print(X[1:5,])
```

Now we split the data into training (80%) and testing sets
```{r echo=F}
train_X = X[1:200,]
train_y = as.matrix(data$y[1:200])

test_X = X[201:250,]
test_y = as.matrix(data$y[201:250])
```


###Forward selection works as follows:
  fit models using all columns of X separately (fit 6 models)
  calculate MSE of all models
  find model with min(MSE)
  append that model predictor to final model formula and remove that predictor from X
  
  Fit models using selected predictor + all columns of X individually (again 6 models)
  repeat the other steps
  
  Do this until model has 3 terms
  
### Model fitting function
Because we'll need to fit multiple models, generate predictions and calculate MSE it might be good to have all of this wrapped in one nice function.
```{r echo=FALSE}
fit_evaluate = function(X_tr, y_tr=train_y, X_ts, y_ts = test_y){
  #estimate parameters
  theta = solve(crossprod(X_tr), crossprod(X_tr, y_tr))
  #predict test data
  predictions = X_ts %*% theta
  #calculate error
  MSE = mean((y_ts - predictions)^2)
  
  return(MSE)
}
```

### Forward selection
General steps in the algorithm:
  Run for loop through all columns of X matrix, fitting model and calculating MSE (fitting 6 models)
  Select model with lowest MSE (on testing data) (hint: function which.min())
  Append that predictor to final model and remove it from X (e.g. X[,-selected_predictor])
  Repeat...it's quite simple really
```{r echo=F}
cat("Running first round of selection\n")
round1 = data.frame(term = colnames(X),
                    MSE = rep(0,6))

for (i in 1:6) {
  round1[i,2] = fit_evaluate(X_tr = train_X[,i], X_ts = test_X[,i])
}

index = which.min(round1$MSE)
best1 = (as.character(round1$term[which.min(round1$MSE)]))
cat("Best model with 1 parameter:", paste0("y ~ ",best1, " + error"), "(MSE=", round1$MSE[index], ")")

final_X_tr = train_X[,index]
final_X_ts = test_X[,index]
train_X2 = train_X[,-index]
test_X2 = test_X[,-index]

#round 2
cat("\nRunning second round of selection\n")
round2 = data.frame(term = paste0(best1, " + ", colnames(train_X2)),
                    MSE = rep(0,5))

for (i in 1:5) {
  round2[i,2] = fit_evaluate(X_tr = cbind(train_X2[,i],final_X_tr), X_ts = cbind(test_X2[,i], final_X_ts))
}

index = which.min(round2$MSE)
best2 = (as.character(round2$term[which.min(round2$MSE)]))
cat("\nBest model with 2 parameters:", paste0("y ~ ",best2, " + error"), "(MSE=", round2$MSE[index], ")")


final_X_tr = cbind(final_X_tr, train_X2[,index])
final_X_ts = cbind(final_X_ts, test_X2[,index])
train_X3 = train_X2[,-index]
test_X3 = test_X2[,-index]

#round 3
cat("\nRunning third round of selection\n")
round3 = data.frame(term = paste0(best2, " + ", colnames(train_X3)),
                    MSE = rep(0,4))

for (i in 1:4) {
    round3[i,2] = fit_evaluate(X_tr = cbind(train_X3[,i],final_X_tr), X_ts = cbind(test_X3[,i], final_X_ts))
}

index = which.min(round3$MSE)
best3 = (as.character(round3$term[which.min(round3$MSE)]))
cat("\nBest model with 3 parameters:", paste0("y ~ ",best3, " + error"), "(MSE=", round3$MSE[index], ")")
```

According to forward selection the best model is: y ~ b1x + b2x^2 + b3x^4

## Information criterion selection

### Adjust fitting function to return AIC instead of MSE
Also now we use the full dataset as we don't need to compute out-of-sample metrics
```{r echo=FALSE}
fit_evaluate = function(X, Y){
  X = as.matrix(X)
  Y = as.matrix(Y)
  #estimate parameters
  theta = solve(crossprod(X), crossprod(X, Y))
  #predict test data
  predictions = X %*% theta
  #calculate error
  residuals = Y - predictions
  sigma_sq = sum(residuals^2)/(nrow(Y) - 1)
  loglik= sum(log(dnorm(data$y, mean = predictions, sd = sqrt(sigma_sq))))

  k = ncol(X)
  AIC = 2*k - 2*loglik
  
  return(AIC)
}
```

Now we construct a vector with all combinations of [1:3] predictors
```{r}
parameters = 1:6
candidates_1 = t(as.matrix(parameters))
candidates_2 = combn(parameters, m = 2)
candidates_3 = combn(parameters, m = 3)
```

Now we can run a for loop through all allowed parameter combinations
```{r echo=F}
# 1 parameter
AIC_results_1 = data.frame(combination = rep(0, ncol(candidates_1)),
                         AIC = rep(0, ncol(candidates_1)))

for (i in 1:ncol(candidates_1)) {
  x = X[,candidates_1[,i]]
  
  AIC_results_1[i, 1] = colnames(X)[i]
  AIC_results_1[i, 2] = fit_evaluate(x, data$y)
}

# 2 parameters
AIC_results_2 = data.frame(combination = rep(0, ncol(candidates_2)),
                         AIC = rep(0, ncol(candidates_2)))

for (i in 1:ncol(candidates_2)) {
  x = X[,candidates_2[,i]]
  
  AIC_results_2[i, 1] = paste(colnames(x), collapse = " + ")
  AIC_results_2[i, 2] = fit_evaluate(x, data$y)
}

# 3 parameters
AIC_results_3 = data.frame(combination = rep(0, ncol(candidates_3)),
                         AIC = rep(0, ncol(candidates_3)))

for (i in 1:ncol(candidates_3)) {
  x = X[,candidates_3[,i]]
  
  AIC_results_3[i, 1] = paste(colnames(x), collapse = " + ")
  AIC_results_3[i, 2] = fit_evaluate(x, data$y)
}

AIC_results = rbind(AIC_results_1, AIC_results_2, AIC_results_3)

best_AIC = AIC_results$combination[which.min(AIC_results$AIC)]
min_AIC = AIC_results$AIC[which.min(AIC_results$AIC)]

cat("According to AIC the best model is:\n",
    "y ~",best_AIC, 
    "\n(AIC =", min_AIC, ")")
```

### Just for fun, let's use BIC as well
```{r echo=F}
fit_evaluate = function(X, Y){
  X = as.matrix(X)
  Y = as.matrix(Y)
  #estimate parameters
  theta = solve(crossprod(X), crossprod(X, Y))
  #predict test data
  predictions = X %*% theta
  #calculate error
  residuals = Y - predictions
  sigma_sq = sum(residuals^2)/(nrow(Y) - 1)
  loglik= sum(log(dnorm(data$y, mean = predictions, sd = sqrt(sigma_sq))))

  k = ncol(X)
  BIC = log(nrow(Y))*k - 2*loglik
  
  return(BIC)
}
```

```{r echo=F}
# 1 parameter
BIC_results_1 = data.frame(combination = rep(0, ncol(candidates_1)),
                           BIC = rep(0, ncol(candidates_1)))

for (i in 1:ncol(candidates_1)) {
  x = X[,candidates_1[,i]]
  
  BIC_results_1[i, 1] = colnames(X)[i]
  BIC_results_1[i, 2] = fit_evaluate(x, data$y)
}

# 2 parameters
BIC_results_2 = data.frame(combination = rep(0, ncol(candidates_2)),
                           BIC = rep(0, ncol(candidates_2)))

for (i in 1:ncol(candidates_2)) {
  x = X[,candidates_2[,i]]
  
  BIC_results_2[i, 1] = paste(colnames(x), collapse = " + ")
  BIC_results_2[i, 2] = fit_evaluate(x, data$y)
}

# 3 parameters
BIC_results_3 = data.frame(combination = rep(0, ncol(candidates_3)),
                           BIC = rep(0, ncol(candidates_3)))

for (i in 1:ncol(candidates_3)) {
  x = X[,candidates_3[,i]]
  
  BIC_results_3[i, 1] = paste(colnames(x), collapse = " + ")
  BIC_results_3[i, 2] = fit_evaluate(x, data$y)
}

BIC_results = rbind(BIC_results_1, BIC_results_2, BIC_results_3)

best_BIC = BIC_results$combination[which.min(BIC_results$BIC)]
min_BIC = BIC_results$BIC[which.min(BIC_results$BIC)]

cat("According to BIC the best model is:\n",
    "y ~",best_BIC, 
    "\n(BIC =", min_BIC, ")")
```

In order, to keep this notebook short (and easy to navigate) we'll explore the best model in next notebook.



